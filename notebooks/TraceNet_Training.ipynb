{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0TehWhEAb1u"
      },
      "source": [
        "# TraceNet - Custom Facial Recognition Training\n",
        "\n",
        "**Final Year Project: Advanced Face Recognition System**\n",
        "\n",
        "This notebook trains a custom facial recognition model from scratch:\n",
        "- **Architecture:** ResNet-50 inspired (TraceNet)\n",
        "- **Loss:** ArcFace (Angular Margin Loss)\n",
        "- **Dataset:** LFW (Labeled Faces in the Wild)\n",
        "- **Target Accuracy:** 95%+ on LFW benchmark\n",
        "\n",
        "---\n",
        "\n",
        "## Setup Instructions:\n",
        "1. **Runtime â†’ Change runtime type â†’ GPU (T4 or better)**\n",
        "2. Run all cells in order\n",
        "3. Training will take ~6-12 hours\n",
        "4. Models are saved to Google Drive automatically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj7bnk7TAb1v"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G27G1FN8Ab1v",
        "outputId": "c07368c5-00c6-43fa-c82f-fd8812a71a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive mounted successfully!\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (to save checkpoints)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directory\n",
        "!mkdir -p /content/drive/MyDrive/TraceNet\n",
        "!mkdir -p /content/drive/MyDrive/TraceNet/checkpoints\n",
        "!mkdir -p /content/drive/MyDrive/TraceNet/logs\n",
        "\n",
        "print(\"âœ… Google Drive mounted successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-jZ18saAb1v",
        "outputId": "1c429c5d-dff7-49de-af5c-2d211e46423c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q opencv-python-headless\n",
        "!pip install -q pillow\n",
        "!pip install -q albumentations\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib\n",
        "!pip install -q tqdm\n",
        "!pip install -q wandb  # Optional: for experiment tracking\n",
        "\n",
        "print(\"âœ… All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE4YDA_CAb1w",
        "outputId": "bcfdb2e7-9201-4db4-ccf2-c093c749bb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "CUDA version: 12.8\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"âš ï¸ WARNING: No GPU detected! Go to Runtime â†’ Change runtime type â†’ GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "if os.path.exists('/content/lfw'):\n",
        "    print(\"ðŸ—‘ï¸ Removing previous incomplete download...\")\n",
        "    shutil.rmtree('/content/lfw')\n",
        "    print(\"âœ… Cleanup complete!\")\n",
        "if os.path.exists('/content/data'):\n",
        "    print(\"ðŸ—‘ï¸ Removing previous data directory...\")\n",
        "    shutil.rmtree('/content/data')\n",
        "    print(\"âœ… Cleanup complete!\")\n",
        "# ====================\n",
        "# STEP 2: Install kagglehub\n",
        "# ====================\n",
        "!pip install -q kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbZUTfz7ERwp",
        "outputId": "c8e2b6ab-d732-4af9-aa3b-d7367759a78c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ—‘ï¸ Removing previous incomplete download...\n",
            "âœ… Cleanup complete!\n",
            "ðŸ—‘ï¸ Removing previous data directory...\n",
            "âœ… Cleanup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddY4ykHaAb1w"
      },
      "source": [
        "## 2. Download & Prepare Dataset\n",
        "\n",
        "**LFW Dataset:**\n",
        "- 13,233 images\n",
        "- 5,749 identities\n",
        "- Perfect for training and benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install kagglehub\n",
        "!pip install -q kagglehub\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Download LFW dataset from Kaggle\n",
        "if not os.path.exists('/content/lfw'):\n",
        "    print(\"Downloading LFW dataset from Kaggle...\")\n",
        "    path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")\n",
        "\n",
        "    print(f\"âœ… Dataset downloaded to: {path}\")\n",
        "\n",
        "    # The dataset has a subfolder 'lfw-deepfunneled' with the actual images\n",
        "    source = os.path.join(path, 'lfw-deepfunneled')\n",
        "\n",
        "    # Copy (not move) because Kaggle cache is read-only\n",
        "    print(f\"Copying dataset from {source} to /content/lfw...\")\n",
        "    shutil.copytree(source, '/content/lfw')\n",
        "\n",
        "    print(\"âœ… LFW dataset ready at /content/lfw\")\n",
        "else:\n",
        "    print(\"âœ… LFW dataset already available!\")\n",
        "\n",
        "# Verify\n",
        "print(f\"\\nTotal images: {sum([len(files) for r, d, files in os.walk('/content/lfw')])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4hS85dCDNyU",
        "outputId": "d7c08a4f-d3b0-4777-cfce-80e684754213"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading LFW dataset from Kaggle...\n",
            "Using Colab cache for faster access to the 'lfw-dataset' dataset.\n",
            "âœ… Dataset downloaded to: /kaggle/input/lfw-dataset\n",
            "Copying dataset from /kaggle/input/lfw-dataset/lfw-deepfunneled to /content/lfw...\n",
            "âœ… LFW dataset ready at /content/lfw\n",
            "\n",
            "Total images: 13233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i88_5tJ4Ab1w"
      },
      "source": [
        "## 3. Model Architecture - TraceNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PkR0Kk2Ab1w",
        "outputId": "9a871895-8523-48fa-f638-25387767f7fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model architecture defined!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with skip connection\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        return F.relu(out)\n",
        "\n",
        "\n",
        "class TraceNet(nn.Module):\n",
        "    \"\"\"Custom Face Recognition Network (ResNet-50 inspired)\"\"\"\n",
        "    def __init__(self, embedding_size=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(3, 2, 1)\n",
        "\n",
        "        # Residual layers\n",
        "        self.layer1 = self._make_layer(64, 64, 3, 1)\n",
        "        self.layer2 = self._make_layer(64, 128, 4, 2)\n",
        "        self.layer3 = self._make_layer(128, 256, 6, 2)\n",
        "        self.layer4 = self._make_layer(256, 512, 3, 2)\n",
        "\n",
        "        # Global average pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Embedding layer\n",
        "        self.fc = nn.Linear(512, embedding_size)\n",
        "        self.bn_fc = nn.BatchNorm1d(embedding_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, 1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.bn_fc(self.fc(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ArcFaceLoss(nn.Module):\n",
        "    \"\"\"ArcFace: Additive Angular Margin Loss\"\"\"\n",
        "    def __init__(self, embedding_size, num_classes, s=64.0, m=0.50):\n",
        "        super().__init__()\n",
        "        self.s = s  # Scale\n",
        "        self.m = m  # Margin\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_size))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        # Normalize\n",
        "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "        weight_norm = F.normalize(self.weight, p=2, dim=1)\n",
        "\n",
        "        # Cosine similarity\n",
        "        cos_theta = F.linear(embeddings, weight_norm).clamp(-1, 1)\n",
        "\n",
        "        # Add margin\n",
        "        theta = torch.acos(cos_theta)\n",
        "        one_hot = torch.zeros_like(cos_theta)\n",
        "        one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
        "\n",
        "        theta_m = theta + self.m * one_hot\n",
        "        output = torch.cos(theta_m) * self.s\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"âœ… Model architecture defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def prepare_dataset():\n",
        "    lfw_dir = Path(\"/content/lfw\")\n",
        "    data_dir = Path(\"/content/data\")\n",
        "\n",
        "    # Ensure previous (potentially empty) data directory is removed\n",
        "    if data_dir.exists():\n",
        "        print(\"ðŸ—‘ï¸ Removing previous data directory to re-prepare...\")\n",
        "        shutil.rmtree(data_dir)\n",
        "        print(\"âœ… Cleanup complete!\")\n",
        "\n",
        "    # Create directories\n",
        "    (data_dir / \"train\").mkdir(parents=True, exist_ok=True)\n",
        "    (data_dir / \"val\").mkdir(parents=True, exist_ok=True)\n",
        "    (data_dir / \"test\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Get all identities with at least 5 images\n",
        "    identities = []\n",
        "    for person_dir in lfw_dir.iterdir():\n",
        "        if person_dir.is_dir():\n",
        "            images = list(person_dir.glob(\"*.jpg\"))\n",
        "            if len(images) >= 5: # Changed from 10 to 5\n",
        "                identities.append((person_dir.name, images))\n",
        "\n",
        "    print(f\"Found {len(identities)} identities with 5+ images\")\n",
        "\n",
        "    # Split: 70% train, 15% val, 15% test\n",
        "    random.shuffle(identities)\n",
        "    train_split = int(0.7 * len(identities))\n",
        "    val_split = int(0.85 * len(identities))\n",
        "\n",
        "    splits = {\n",
        "        'train': identities[:train_split],\n",
        "        'val': identities[train_split:val_split],\n",
        "        'test': identities[val_split:]\n",
        "    }\n",
        "\n",
        "    for split_name, split_data in splits.items():\n",
        "        for person_name, images in tqdm(split_data, desc=f\"Preparing {split_name}\"):\n",
        "            person_dir = data_dir / split_name / person_name\n",
        "            person_dir.mkdir(exist_ok=True)\n",
        "\n",
        "            for img in images:\n",
        "                shutil.copy(img, person_dir / img.name)\n",
        "\n",
        "    print(\"\\nâœ… Dataset prepared!\")\n",
        "    print(f\"   Train: {len(splits['train'])} identities\")\n",
        "    print(f\"   Val: {len(splits['val'])} identities\")\n",
        "    print(f\"   Test: {len(splits['test'])} identities\")\n",
        "\n",
        "prepare_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21SQcznPG2-g",
        "outputId": "47e408d1-8ecd-4e49-b78c-ef355cc4b0c1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 identities with 5+ images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing train: 0it [00:00, ?it/s]\n",
            "Preparing val: 0it [00:00, ?it/s]\n",
            "Preparing test: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Dataset prepared!\n",
            "   Train: 0 identities\n",
            "   Val: 0 identities\n",
            "   Test: 0 identities\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeD_uSyFAb1w"
      },
      "source": [
        "## 4. Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXt9PhtpAb1x",
        "outputId": "87b603df-d8aa-4b51-f258-82c4bc1db8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Datasets created!\n",
            "   Train: 0 images, 0 identities\n",
            "   Val: 0 images, 0 identities\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        # Load all images\n",
        "        for idx, person in enumerate(sorted(os.listdir(root_dir))):\n",
        "            person_dir = os.path.join(root_dir, person)\n",
        "            if not os.path.isdir(person_dir):\n",
        "                continue\n",
        "\n",
        "            for img_name in os.listdir(person_dir):\n",
        "                if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    img_path = os.path.join(person_dir, img_name)\n",
        "                    self.samples.append((img_path, idx))\n",
        "\n",
        "        self.num_classes = len(set([s[1] for s in self.samples]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            # Return next sample if error\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = FaceDataset('/content/data/train', transform=train_transform)\n",
        "val_dataset = FaceDataset('/content/data/val', transform=val_transform)\n",
        "\n",
        "print(f\"âœ… Datasets created!\")\n",
        "print(f\"   Train: {len(train_dataset)} images, {train_dataset.num_classes} identities\")\n",
        "print(f\"   Val: {len(val_dataset)} images, {val_dataset.num_classes} identities\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "zjFQc0bqAb1x",
        "outputId": "45bfd634-ec66-409d-cbb9-6e89d427f7e9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2917260966.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m  \u001b[0;31m# Adjust based on GPU memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_loader = DataLoader(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "# Create data loaders\n",
        "BATCH_SIZE = 64  # Adjust based on GPU memory\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"âœ… DataLoaders created!\")\n",
        "print(f\"   Batches per epoch: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f3Ji64WAb1x"
      },
      "source": [
        "## 5. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xi4oO4AAb1x"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "EMBEDDING_SIZE = 512\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 100\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/TraceNet/checkpoints'\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model\n",
        "model = TraceNet(embedding_size=EMBEDDING_SIZE).to(device)\n",
        "criterion = ArcFaceLoss(\n",
        "    embedding_size=EMBEDDING_SIZE,\n",
        "    num_classes=train_dataset.num_classes,\n",
        "    s=64.0,\n",
        "    m=0.50\n",
        ").to(device)\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(criterion.parameters()),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=NUM_EPOCHS\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Training setup complete!\")\n",
        "print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwP-Zn4bAb1x"
      },
      "source": [
        "## 6. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7-ylcKbAb1x"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "def train_epoch(epoch):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward\n",
        "        embeddings = model(images)\n",
        "        outputs = criterion(embeddings, labels)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Update progress\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def validate():\n",
        "    model.eval()\n",
        "    criterion.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            embeddings = model(images)\n",
        "            outputs = criterion(embeddings, labels)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, val_acc, is_best=False):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'criterion_state_dict': criterion.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_accuracy': val_acc,\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "    # Save latest\n",
        "    torch.save(checkpoint, f'{CHECKPOINT_DIR}/latest.pth')\n",
        "\n",
        "    # Save best\n",
        "    if is_best:\n",
        "        torch.save(checkpoint, f'{CHECKPOINT_DIR}/best.pth')\n",
        "        print(f\"âœ… New best model saved! Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "def plot_history():\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss\n",
        "    ax1.plot(history['train_loss'], label='Train')\n",
        "    ax1.plot(history['val_loss'], label='Validation')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training & Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Accuracy\n",
        "    ax2.plot(history['train_acc'], label='Train')\n",
        "    ax2.plot(history['val_acc'], label='Validation')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.set_title('Training & Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{CHECKPOINT_DIR}/../logs/training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "print(\"âœ… Training functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwtgmMuzAb1x"
      },
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "print(\"ðŸš€ Starting training...\\n\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(epoch)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate()\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    is_best = val_acc > best_val_acc\n",
        "    if is_best:\n",
        "        best_val_acc = val_acc\n",
        "    save_checkpoint(epoch, val_acc, is_best)\n",
        "\n",
        "    # Plot progress every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        plot_history()\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R23MuSGTAb1x"
      },
      "source": [
        "## 7. Final Evaluation & Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdE2_MPkAb1x"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(f'{CHECKPOINT_DIR}/best.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"âœ… Best model loaded (Epoch {checkpoint['epoch']+1})\")\n",
        "print(f\"   Validation Accuracy: {checkpoint['val_accuracy']:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EltnjRfSAb1x"
      },
      "outputs": [],
      "source": [
        "# Export for deployment (model only, no loss)\n",
        "deployment_model = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'embedding_size': EMBEDDING_SIZE,\n",
        "    'accuracy': checkpoint['val_accuracy'],\n",
        "    'epoch': checkpoint['epoch']\n",
        "}\n",
        "\n",
        "torch.save(deployment_model, f'{CHECKPOINT_DIR}/tracenet_deployment.pth')\n",
        "print(\"âœ… Deployment model exported to Google Drive!\")\n",
        "print(f\"   Path: {CHECKPOINT_DIR}/tracenet_deployment.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSnTTP7ZAb1x"
      },
      "outputs": [],
      "source": [
        "# Test inference\n",
        "import numpy as np\n",
        "\n",
        "def extract_embedding(image):\n",
        "    \"\"\"Extract embedding from image\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = Image.fromarray(image)\n",
        "        image_tensor = val_transform(image).unsqueeze(0).to(device)\n",
        "        embedding = model(image_tensor)\n",
        "        embedding = F.normalize(embedding, p=2, dim=1)\n",
        "    return embedding.cpu().numpy()[0]\n",
        "\n",
        "# Test with sample image\n",
        "test_img_path = train_dataset.samples[0][0]\n",
        "test_img = Image.open(test_img_path).convert('RGB')\n",
        "embedding = extract_embedding(test_img)\n",
        "\n",
        "print(f\"âœ… Embedding extracted successfully!\")\n",
        "print(f\"   Shape: {embedding.shape}\")\n",
        "print(f\"   L2 Norm: {np.linalg.norm(embedding):.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-7zbOjyAb1x"
      },
      "source": [
        "## 8. Download Model to Local System\n",
        "\n",
        "The trained model is saved in your Google Drive at:\n",
        "- **Best model:** `/content/drive/MyDrive/TraceNet/checkpoints/best.pth`\n",
        "- **Deployment model:** `/content/drive/MyDrive/TraceNet/checkpoints/tracenet_deployment.pth`\n",
        "\n",
        "You can download it directly from Google Drive or use the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpOQx3J7Ab1x"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download deployment model\n",
        "files.download(f'{CHECKPOINT_DIR}/tracenet_deployment.pth')\n",
        "print(\"âœ… Model download started!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5SslchCAb1x"
      },
      "source": [
        "## 9. Integration Instructions\n",
        "\n",
        "To use this model in your TraceIQ backend:\n",
        "\n",
        "1. Download `tracenet_deployment.pth` to your project:\n",
        "   ```bash\n",
        "   cp tracenet_deployment.pth backend/models/\n",
        "   ```\n",
        "\n",
        "2. Load the model in your inference service:\n",
        "   ```python\n",
        "   checkpoint = torch.load('models/tracenet_deployment.pth')\n",
        "   model = TraceNet(embedding_size=512)\n",
        "   model.load_state_dict(checkpoint['model_state_dict'])\n",
        "   model.eval()\n",
        "   ```\n",
        "\n",
        "3. Extract embeddings:\n",
        "   ```python\n",
        "   embedding = model(preprocessed_image)\n",
        "   embedding = F.normalize(embedding, p=2, dim=1)\n",
        "   ```\n",
        "\n",
        "**Expected Performance:**\n",
        "- Validation Accuracy: `{best_val_acc:.2f}%`\n",
        "- Inference Time: ~5-10ms per face (GPU)\n",
        "- Embedding Size: 512 dimensions\n",
        "\n",
        "---\n",
        "\n",
        "**ðŸŽ“ Congratulations! You've successfully trained your custom facial recognition model!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}